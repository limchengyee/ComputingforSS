---
title: "Titanic Decision Trees and Random Forests"  
author: "Cheng Yee Lim"  
date: "February 14, 2017"  
output: github_document
---

```{r, include = FALSE}

knitr::opts_chunk$set(
  cache = TRUE, 
  message = FALSE, 
  warning = FALSE
  )

library(tidyverse)
library(modelr)
library(broom)
library(titanic)
library(tree)
library(knitr)
library(caret)

logit2prob <- function(x){
  exp(x) / (1 + exp(x))
} #logit2prob function

options(na.action = na.warn)

theme_set(theme_bw())

```

```{r} 
set.seed(1234) #reproducible results

titanic_split <- resample_partition(titanic_train,
                                    c(test = 0.3, train = 0.7)) #split 70/30 training/test set

titanic_train <- titanic_split$train %>%
  tbl_df() %>%
  select(Survived, Sex, Age, Parch, Fare) %>%
  na.omit() %>%
  mutate_each(funs(as.factor), Survived, Sex) #create clean dataset for rf analysis

titanic_test <- titanic_split$test %>%
  tbl_df() %>%
  select(Survived, Sex, Age, Parch, Fare) %>%
  na.omit() %>%
  mutate_each(funs(as.factor), Survived, Sex) #create clean dataset for rf analysis

titanic_train <- titanic_train %>%
  select(Survived, Sex, Age, Parch, Fare) %>%
  na.omit() %>%
  mutate_each(funs(as.factor), Survived, Sex) #create clean dataset for rf analysis

```

##Logistic Regression Models
Estimate three different logistic regression models using the training set with Survived as the response variable. Calculate the test set accuracy rate for each logistic regression model. Which performs the best?

###Model 1 - Age, Sex 
```{r}
age_sex <- glm(Survived ~ Age + Sex, 
               data = titanic_train, 
               family = binomial)
summary(age_sex)
```

###Model 2 - Age, Sex, Fare
```{r}
fare <- glm(Survived ~ Age + Sex + Fare, 
               data = titanic_train, 
               family = binomial)
summary(fare)
```

###Model 3 - Age, Sex, Fare, Parents/Children
```{r}
parents <- glm(Survived ~ Age + Sex + Fare + Parch, 
               data = titanic_train, 
               family = binomial)
summary(parents)
```

###Model Accuracy 
```{r log_acc}

test_accuracy <- function(model) {
  value <- titanic_test %>%
    add_predictions(model) %>%
    mutate(pred = logit2prob(pred),
           pred = as.numeric(pred > .5))
    mean(value$Survived == value$pred, na.rm = TRUE)
} #calculating test set accuracy for log reg 

test_accuracy(age_sex)
test_accuracy(fare)
test_accuracy(parents)


```

| Model               | Test Set Accuracy     | 
| -------------       |:-------------:        | 
| Age, sex            | 0.7511962             | 
| Age, sex, fare      |  0.7416268            | 
| Age, sex, fare, parents/children |0.7559809 | 

The logistic model with age, sex, fare and parents/children performed best, followed by the model with age and sex only, and lastly, the model with age, sex and fare. 

##Random Forests 
Now estimate three random forest models using the same model specifications as the logistic regression models. Generate variable importance plots for each random forest model. Which variables seem the most important?
Calculate the test set accuracy rate for each random forest model. Which performs the best?

###Training with Age and Sex 
```{r rf1}
rf_age_sex <- train(Survived ~ Age + Sex, data = titanic_train,
                   method = "rf",
                   ntree = 500,
                   trControl = trainControl(method = "oob"))

rf_age_sex$finalModel
```
In the model with age and sex, sex was more important than age for surivval.  

###Training with Age, Sex and Fare
```{r rf2}
rf_fare <- train(Survived ~ Age + Sex + Fare, data = titanic_train,
                   method = "rf",
                   ntree = 500,
                   trControl = trainControl(method = "oob"))

rf_fare$finalModel

```

###Training with Age, Sex, Fare and Number of Parents/Children aboard
```{r rf3}
rf_par <- train(Survived ~ Age + Sex + Fare + Parch, data = titanic_train,
                   method = "rf",
                   ntree = 500,
                   trControl = trainControl(method = "oob"))

rf_par$finalModel
```

##Variable Importance 

```{r}
varImpPlot(rf_age_sex$finalModel)
```

In the model with age and sex, sex was more important than age for surivval.  

```{r}
varImpPlot(rf_fare$finalModel)

```

In the model with age, sex, and fare, fare was more important than age and sex for survival. Sex was slightly more important than age for survival. 

```{r}
varImpPlot(rf_par$finalModel)
```

In the model with age, sex, fare and parents/children aboard, fare and sex were the most important variables, followed by age, and number of parents/children aboard in predicting survival. 

##Test Set Accuracy
```{r}
test_acc_rf <- function(model_rf) {
  value <- titanic_test %>% 
  bind_cols(predict(model_rf, newdata = titanic_test, type = "prob")) %>% 
  rename(prob_dead = `0`,
         prob_survive = `1`) %>% 
  mutate(pred_survive = ifelse(prob_survive>0.5, 1, 0))
  mean(value$Survived == value$pred_survive, na.rm = TRUE)
} #calculating test set accuracy for rf 

test_acc_rf(rf_age_sex)
test_acc_rf(rf_fare)
test_acc_rf(rf_par)
```

| Random Forest Model | Test Set Accuracy     | 
| -------------       |:-------------:        | 
| Age, sex            | 0.7615894             | 
| Age, sex, fare      |  0.8013245            | 
| Age, sex, fare, parents/children |0.8344371 | 

The model with age, sex, fare and number of parents/children aboard was the best in predicting survival, with the highest test set accuracy of 0.834. The model with age, sex and fare was the second best, with a test set accuracy of 0.801. Lastly, the model with only sex and age had a test set accuracy of 0.762. 

##Logistic Regression vs. Random Forests 
Compare the test set accuracy rates between the logistic regression and random forest models. Which method (logistic regression or random forest) performed better?

|Type | Model | Test Set Accuracy     | 
| -------------       |-------------  |:-------------:| 
| Logistic Regression | Age, sex            | 0.7511962             | 
| Logistic Regression | Age, sex, fare      |  0.7416268            | 
| Logistic Regression | Age, sex, fare, parents/cihldren aboard |  0.7559809 | 
| Random Forest | Age, sex | 0.7615894 | 
| Random Forest | Age, sex, fare | 0.8013245 | 
| Random Forest | Age, sex, fare, parents/children aboard |0.8344371 | 

Random Forests have performed better than logistic regression models for all models. In fact, a poorer specification (i.e. a model with age and sex only) in random forests was already a better model in predicting survival than all models estimated with a logistic regression. 
