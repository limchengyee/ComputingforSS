---
title: "Resampling methods and distributed learning"  
author: "Cheng Yee Lim"  
date: "February 20, 2017"  
output: github_document
---

```{r, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(
  cache = TRUE, 
  message = FALSE, 
  warning = FALSE
  )

library(tidyverse)
library(modelr)
library(broom)
library(knitr)
library(pander)
library(titanic)
library(sparklyr)
library(purrr)
library(profvis)
sc <- spark_connect(master = "local")
options(na.action = na.omit)
set.seed(1234)
theme_set(theme_minimal())
```

##Question 1: Single Linear Regression 
```{r}
#function and import 
mse <- function(model, data) {
  x <- modelr:::residuals(model, data)
  mean(x ^ 2, na.rm = TRUE)
} #function to calculate MSE

joe_lm <- function(df){
  lm <- lm(biden ~ . , data = df)
} #lm with all variables as independent vars

joe <- read.csv("./data/biden.csv") %>%
  na.omit() #import data, remove missing obs
```

###Estimate the model using the entire set of observations and calculate the MSE.
```{r linear_reg}
pander(summary(joe_lm(joe))) #results of linear regression
mse(joe_lm(joe), joe) #calculate mse
```

####Estimate the model using the validation set approach (70/30 training/test split) and calculate the MSE.
```{r}
joe_split <- resample_partition(joe, c(test = 0.7, train = 0.3))
joe_train <- joe_split$train %>%
  tbl_df()
joe_test <- joe_split$test %>%
  tbl_df()
pander(summary(joe_lm(joe_train))) #results of 70/30 training/test split
mse(joe_lm(joe_train), joe_test)
```

####Estimate the model using the LOOCV approach and calculate the MSE.
```{r}
LOOCV <- function(df, n){
  #create loocv data
  loocv_data <- crossv_kfold(df, n)
  #regress every loocv datapoint
  loocv_mods <- map(loocv_data$train, ~ lm(biden ~ . , data = .))
  #calculate mse for every loocv datapoint
  loocv_mse <- map2_dbl(loocv_mods, loocv_data$test, mse)
  #mse of loocv is the average of every mse calculated
  print(mean(loocv_mse, na.rm = TRUE))
} #function to calculate mse for k-fold loocv approach, where max k = nrow(df)

loocv_joe <- LOOCV(joe, nrow(joe))
```

####Estimate the model using the 10-fold CV approach and calculate the MSE.
```{r}
kfold_joe <- LOOCV(joe, 10)
```

####Report on any discrepancies or differences between the estimated MSEs and briefly explain why they may differ from one another.

| Model               | MSE                                     | 
| -------------       |:-------------:                          | 
| 100% train          | `r mse(joe_lm(joe), joe)`               | 
| 70% train, 30% test | `r mse(joe_lm(joe_train), joe_test)`    | 
| LOOCV               | `r loocv_joe`                           |
| 10-fold CV          | `r kfold_joe`                           |

As expected, the estimated MSE using all the observations (baseline) in the dataset is smaller then LOOCV and 10-fold CV as the regression. This is because the model trained with all the observations so MSE at all points is minimized. Whenever we conduct LOOCV or 10-fold CV, we train with less observations, n-1 observations for LOOCV and 90% of the observations for 10-fold CV. Thus, estimated MSE will be smallest for 100% training model, followed by LOOCV model and 10-fold CV model. The estimated MSE will always be the same for LOOCV. But the estimated MSE will vary for k-fold CV models and training-test split models depending on the value of k and the training-test split in the model. 

In the case of 70/30 training/test split model, the estimated MSE is slightly smaller than the other models by chance. We will probably get an estimated MSE closer to the baseline using a different seed. 

##Question 2: Titanic Logistic Regression Models 

####Estimate the models using the entire set of obserations and calculate the MSE of each model.
```{r}
mse.glm <- function (model, data){
  residuals.glm <- function(model, data) {
    modelr:::response(model, data) - stats::predict(model, data, type = "response")
  }
  
  x <- residuals(model, data)
  mean(x^2, na.rm = TRUE)
} #mse.glm function 

```

###Model 1 - Age, Sex 
```{r}
age_sex <- glm(Survived ~ Age + Sex, 
               data = titanic_train, 
               family = binomial)
pander(summary(age_sex))

mse.glm(age_sex, titanic_train)
```

###Model 2 - Age, Sex, Fare
```{r}
fare <- glm(Survived ~ Age + Sex + Fare, 
               data = titanic_train, 
               family = binomial)
pander(summary(fare))
mse.glm(fare, titanic_train)
```

###Model 3 - Age, Sex, Fare, Parents/Children
```{r}
parents <- glm(Survived ~ Age + Sex + Fare + Parch, 
               data = titanic_train, 
               family = binomial)
pander(summary(parents))
mse.glm(parents, titanic_train)
```


####Estimate the models using 10-fold CV and calculate the MSE of each model. How do these values compare to the original estimates using the full dataset? Which model performs the best?
```{r}
#split the data
titanic_split <- resample_partition(titanic_train, c(test = 0.7, train = 0.3))
t_train <- titanic_split$train %>%
  na.omit() %>%
  tbl_df()
t_test <- titanic_split$test %>%
  na.omit() %>%
  tbl_df()

#model1 
LOOCV_agesex  <- function(df, n){
  loocv_data <- crossv_kfold(df, n)
  loocv_mods <- map(loocv_data$train, ~ glm(Survived ~ Age + Sex, family = binomial, data = .))
  loocv_mse <- map2_dbl(loocv_mods, loocv_data$test, mse.glm)
  mean(loocv_mse, na.rm = TRUE)
} #function to calculate mse for k-fold loocv approach for model 1

kfold_agesex <- LOOCV_agesex(t_train, 10)

#model2
LOOCV_fare  <- function(df, n){
  loocv_data <- crossv_kfold(df, n)
  loocv_mods <- map(loocv_data$train, ~ glm(Survived ~ Age + Sex + Fare, family = binomial, data = .))
  loocv_mse <- map2_dbl(loocv_mods, loocv_data$test, mse.glm)
  mean(loocv_mse, na.rm = TRUE)
} #function to calculate mse for k-fold loocv approach for model2

kfold_fare <- LOOCV_fare(t_train, 10)

#model3
LOOCV_par  <- function(df, n){
  loocv_data <- crossv_kfold(df, n)
  loocv_mods <- map(loocv_data$train, ~ glm(Survived ~ Age + Sex + Fare + Parch, family = binomial, data = .))
  loocv_mse <- map2_dbl(loocv_mods, loocv_data$test, mse.glm)
  mean(loocv_mse, na.rm = TRUE)
} #function to calculate mse for k-fold loocv approach for model3

kfold_par <- LOOCV_par(t_train, 10)
```

|Type | 100% Training MSE | 10-fold CV MSE | 
| ------------- |:-------------:|:-------------:| 
| Age, sex    |`r mse.glm(age_sex, titanic_train)` |`r kfold_agesex` | 
| Age, sex, fare |`r mse.glm(fare, titanic_train)` |`r kfold_fare`| 
| Age, sex, fare, parents |`r mse.glm(parents, titanic_train)`| `r kfold_par` | 

The MSE calculated for all three models using 10-fold CV are smaller than the MSE calculated for all three models without cross validation. Model 3 performs best in both methods. 

####Take the model that performs the best and estimate bootstrap standard errors for the parameters. Are there significant differences between the standard errors from the original model trained on all the data vs. the bootstrap estimates?

```{r bootstrap}
library(titanic)
# basic model
titanic_glm <- glm(Survived ~ Age + Sex + Fare + Parch, data = titanic_train, family = binomial)
pander(summary(titanic_glm))

#bootstrap
t_train %>%
  modelr::bootstrap(1000) %>%
  mutate(model = map(strap, ~ glm(Survived ~ Age + Sex + Fare + Parch, data = .,
                                  family = binomial)),
         coef = map(model, tidy)) %>%
  unnest(coef) %>%
  group_by(term) %>%
  summarize(est.boot = mean(estimate),
            se.boot = sd(estimate, na.rm = TRUE)) %>%
  kable()
```

There are significant differences between the standard errors from the original model trained on all the data and the bootstrap estimated standard errors. The estimated standard errors from the bootstrap are significantly larger than the standard errors from the original model. 

##Question 3: Sparklyr 
Take the model specification for your best-performing model from the Titanic problem, and estimate it using the random forest, decision tree, and logistic regression machine learning algorithms in sparklyr. Calculate the accuracy and AUC metrics for each model. Which algorithm performs the best?

```{r sparklyr}
titanic_glm <- glm(Survived ~ Age + Sex + Fare + Parch, data = titanic_train, family = binomial)

#load the data 
partition <- copy_to(sc, titanic::titanic_train, "titanic", overwrite = TRUE) %>%
#tidy the data and transformations
  filter(!is.na(Survived) & !is.na(Age) & !is.na(Sex) & !is.na(Fare) & !is.na(Parch)) %>%
  sdf_register("titanic2") %>%
#train-validation split
  mutate(Survived = as.numeric(Survived), 
         Age = as.numeric(Age), 
         Fare = as.numeric(Fare), 
         Parch = as.numeric(Parch)) %>%
  select(Survived, Sex, Age, Parch, Fare) %>%
  sdf_partition(train = 0.75, test = 0.25, seed = 1234)

#create table references 
train_tbl <- partition$train
test_tbl <- partition$test

#train the models 
# Logistic reg
ml_formula <- formula(Survived ~ Age + Sex + Fare + Parch)
ml_log <- ml_logistic_regression(train_tbl, ml_formula)

# Decision Tree
ml_dt <- ml_decision_tree(train_tbl, ml_formula)

# Random Forest
ml_rf <- ml_random_forest(train_tbl, ml_formula)

#validation data 
# Bundle the models into a single list object
ml_models <- list(
  "Logistic" = ml_log,
  "Decision Tree" = ml_dt,
  "Random Forest" = ml_rf
)

# Create a function for scoring
score_test_data <- function(model, data=test_tbl){
  pred <- sdf_predict(model, data)
  select(pred, Survived, prediction)
}

# Score all the models
ml_score <- map(ml_models, score_test_data)

# Function for calculating accuracy
calc_accuracy <- function(data, cutpoint = 0.5){
  data %>% 
    mutate(prediction = if_else(prediction > cutpoint, 1.0, 0.0)) %>%
    ml_classification_eval("prediction", "Survived", "accuracy")
}

# Calculate AUC and accuracy
perf_metrics <- data_frame(
  model = names(ml_score),
  AUC = 100 * map_dbl(ml_score, ml_binary_classification_eval, "Survived", "prediction"),
  Accuracy = 100 * map_dbl(ml_score, calc_accuracy))
kable(perf_metrics)

# Plot results
gather(perf_metrics, metric, value, AUC, Accuracy) %>%
  ggplot(aes(reorder(model, value), value, fill = metric)) + 
  geom_bar(stat = "identity", position = "dodge") + 
  coord_flip() +
  labs(title = "Performance metrics",
       x = NULL,
       y = "Percent")

```

The decision tree algorithm performs the best in both accuracy and AUC. The random forest algorithm trails closely behind. 